
<!-- README.md is generated from README.Rmd. Please edit that file -->

# prisa: Prediction-error Robust Inference for Statistical Analysis

<!-- badges: start -->

<!-- badges: end -->

The goal of prisa is to facilitate statistical analysis by providing
tools for robust inference in regression models, particularly when
dealing with variables generated by machine learning (ML) or large
language models (LLMs).

## Installation

You can install the development version of prisa from
[GitHub](https://github.com/) with:

``` r
# install.packages("pak")
pak::pak("soichiroy/prisa")
```

## Example: Regression analysis with ML/LLM generated variables

``` r
library(prisa)
library(dplyr)
#> 
#> Attaching package: 'dplyr'
#> The following objects are masked from 'package:stats':
#> 
#>     filter, lag
#> The following objects are masked from 'package:base':
#> 
#>     intersect, setdiff, setequal, union
```

## Data Example

As an example, we use a simulated dataset. In this simulated dataset, we
set the number of “labeled” observations to 200, and the number of
“unlabeled” observations to 5,000.

``` r
# Setup
set.seed(2025)
n_ell <- 200
n_u <- 5000
n_all <- n_ell + n_u

# Covariates
X1 <- rnorm(n = n_all, mean = 0, sd = 1)
X2 <- rnorm(n = n_all, mean = 0, sd = 1)

# Treatment indicator
logistic <- function (x) exp(x) / (1 + exp(x)) 
prob_d <- logistic(-0.3 + 0.4 * X1 + 0.2 * X2)
D <- rbinom(n = n_all, size = 1, prob = prob_d)

# True outcome model for linear regression model
Y <- 0.5 + D + X1 + 1.5 * X2 + rnorm(n = n_all, mean = 0, sd = 1)

# Create a proxy variable. 
Y_proxy_1 <- Y + rnorm(n = n_all, mean = D * X1 + (1 - D) * X2, sd = 1)

# Create a data frame
df <- data.frame(
  Y = Y,
  Y_proxy_1 = Y_proxy_1,
  D = D,
  X1 = X1,
  X2 = X2,
  is_labeled = c(rep(1, n_ell), rep(0, n_u))
) |> 
  dplyr::mutate(Y = dplyr::if_else(is_labeled == 1, Y, NA_real_))
```

## Prepare Analysis Functions

We are interested in estimating the impact of
![D](https://latex.codecogs.com/png.image?%5Cdpi%7B110%7D&space;%5Cbg_white&space;D
"D") on the true outcome
![Y](https://latex.codecogs.com/png.image?%5Cdpi%7B110%7D&space;%5Cbg_white&space;Y
"Y"). The main model is therefore regress
![Y](https://latex.codecogs.com/png.image?%5Cdpi%7B110%7D&space;%5Cbg_white&space;Y
"Y") on
![D](https://latex.codecogs.com/png.image?%5Cdpi%7B110%7D&space;%5Cbg_white&space;D
"D") and other covariates:   
![&#10;Y\_{i} = \\alpha + \\beta D\_{i} + \\gamma\_{1}X\_{i1} +
\\gamma\_{2}X\_{i2} +
\\epsilon\_{i}&#10;](https://latex.codecogs.com/png.image?%5Cdpi%7B110%7D&space;%5Cbg_white&space;%0AY_%7Bi%7D%20%3D%20%5Calpha%20%2B%20%5Cbeta%20D_%7Bi%7D%20%2B%20%5Cgamma_%7B1%7DX_%7Bi1%7D%20%2B%20%5Cgamma_%7B2%7DX_%7Bi2%7D%20%2B%20%5Cepsilon_%7Bi%7D%0A
"
Y_{i} = \\alpha + \\beta D_{i} + \\gamma_{1}X_{i1} + \\gamma_{2}X_{i2} + \\epsilon_{i}
")  
where
![\\beta](https://latex.codecogs.com/png.image?%5Cdpi%7B110%7D&space;%5Cbg_white&space;%5Cbeta
"\\beta") is the parameter of interest.

We specify the analysis function for the main model: `fn_true_lm`. The
analysis function must take the data frame as an input and return the
quantity of interest with other coefficient estimates.

``` r
# Estimating the linear regression coefficient
# Main analysis is to regress the labeled outcome on treatment and covariates
fn_true_lm <- function (df) {
  fit <- lm(Y ~ D + X1 + X2, data = df)
  return(fit$coef)
}
```

When the number of coefficients is large (e.g., many fixed effects), we
recommend selectively returning parameters that are of direct interest.

The Prediction-error robust inference (PERI) allows us to leverage
additional proxy outcomes in improving the analysis based on the true
outcome.

We specify the proxy model, which regresses the proxy outcome
![Y\_{\\text{prox}}](https://latex.codecogs.com/png.image?%5Cdpi%7B110%7D&space;%5Cbg_white&space;Y_%7B%5Ctext%7Bprox%7D%7D
"Y_{\\text{prox}}") on
![D](https://latex.codecogs.com/png.image?%5Cdpi%7B110%7D&space;%5Cbg_white&space;D
"D") and other covariates:   
![&#10;Y\_{\\text{prox},i} = \\eta\_{0} + \\eta\_{1}D\_{i} +
\\eta\_{2}X\_{i1} + \\eta\_{3}X\_{i2} +
&#10;\\varepsilon\_{i}&#10;](https://latex.codecogs.com/png.image?%5Cdpi%7B110%7D&space;%5Cbg_white&space;%0AY_%7B%5Ctext%7Bprox%7D%2Ci%7D%20%3D%20%5Ceta_%7B0%7D%20%2B%20%5Ceta_%7B1%7DD_%7Bi%7D%20%2B%20%5Ceta_%7B2%7DX_%7Bi1%7D%20%2B%20%5Ceta_%7B3%7DX_%7Bi2%7D%20%2B%20%0A%5Cvarepsilon_%7Bi%7D%0A
"
Y_{\\text{prox},i} = \\eta_{0} + \\eta_{1}D_{i} + \\eta_{2}X_{i1} + \\eta_{3}X_{i2} + 
\\varepsilon_{i}
")  

The analysis function for the proxy model takes the identical structure:
the function takes a data frame as an argument and returns coefficients.
In the proxy model function, a vector valued return is allowed.

``` r
# Proxy model is to regress proxy variables on the same set of variables as the
# main model.  We use both proxy variables.
fn_proxy_lm <- function(df) {
  fit1 <- lm(Y_proxy_1 ~ D + X1 + X2, data = df)
  return(fit1$coef)
}
```

## Apply Prediction-error Robust Inference

We apply the PERI method by calling `prisa()` function:

``` r
## With a model for Y_proxy_1
fit_lm <- prisa(
  main_model = fn_true_lm,
  proxy_model = fn_proxy_lm,
  data = df,
  labeled_set_var_name = "is_labeled",
  options = SetOptions(
    n_boot = 500,
    use_full = TRUE,
    use_parallel = FALSE
  )
)
```

In the `prisa()` call, we can specify various options via the
`SetOptions()` function. For example, `n_boot` controls the number of
bootstrap iterations. We recommend setting the value low for a
development phase and increase the number once you want to get accurate
estimates.

Once the estimation is complete, we can view the summary of estimates:

``` r
summary(fit_lm)
#> 
#> ── Prediction-error Robust Inference (prisa) Results ───────────────────────────
#> 
#> ── Main Estimates ──
#> 
#>             estimate std_err ci_lower_95 ci_upper_95  elss
#> (Intercept)   0.6072 0.06685      0.4761      0.7382 402.6
#> D             0.9180 0.10815      0.7061      1.1300 326.1
#> X1            1.0086 0.06017      0.8906      1.1265 300.4
#> X2            1.4922 0.05275      1.3888      1.5956 318.1
#> ── Additional Information ──
#> ── Efficiency Gain from prisa
#> • Effective Labeled Sample Size (ELSS): 402.595, 326.142, 300.356, and 318.094
#> • Contribution from the unlabeled set: 202.6, 126.1, 100.4, and 118.1
#> • Standard Error Reduction: -30, -22, -18, and -21%
#> 
#> ── Data
#> • n_labeled_data: 200
#> • n_full_data: 5200
#> • proportion_of_coded_obs: 0.0385
#> 
#> ── Labeled Only Estimates
#>             estimate std_err ci_lower_95 ci_upper_95 elss
#> (Intercept)   0.5616 0.09485      0.3757      0.7475  200
#> D             0.9478 0.13811      0.6771      1.2185  200
#> X1            1.0560 0.07373      0.9115      1.2005  200
#> X2            1.5231 0.06653      1.3927      1.6535  200
```

A quick way to obtain estimates is to call `get_estimates()` function:

``` r
get_estimates(fit_lm)
#> # A tibble: 8 × 7
#>   variable    estimator    estimate std_err ci_lower_95 ci_upper_95  elss
#>   <chr>       <chr>           <dbl>   <dbl>       <dbl>       <dbl> <dbl>
#> 1 (Intercept) prisa           0.607  0.0669       0.476       0.738  403.
#> 2 D           prisa           0.918  0.108        0.706       1.13   326.
#> 3 X1          prisa           1.01   0.0602       0.891       1.13   300.
#> 4 X2          prisa           1.49   0.0528       1.39        1.60   318.
#> 5 (Intercept) labeled_only    0.562  0.0948       0.376       0.747  200 
#> 6 D           labeled_only    0.948  0.138        0.677       1.22   200 
#> 7 X1          labeled_only    1.06   0.0737       0.912       1.20   200 
#> 8 X2          labeled_only    1.52   0.0665       1.39        1.65   200
```

## Combine Multiple Proxy Variables

When multiple proxy variables are available (for example multiple
responses from a LLM is available), we can combine estimates together.

As an illustration, we simulate a second proxy variable using a
different measurement error mechanism. In addition, we specify another
analysis function for the second proxy variable

``` r
# Simulate the second proxy for an illustration
df$Y_proxy_2 <- round(Y / 4) * 4 + rnorm(n = n_all, mean = 0, sd = 0.5)

# Define the second analysis function for the proxy outcome
fn_proxy_lm_2 <- function(df) {
  fit1 <- lm(Y_proxy_2 ~ D + X1 + X2, data = df)
  fit1$coef
}

# Define a "meta" analysis function that runs two analysis function for the 
# proxy outcomes.
fn_proxy_lm_run <- function(df) {
  fit1 <- fn_proxy_lm(df)
  fit2 <- fn_proxy_lm_2(df)
  c(fit1, fit2)
}
```

Now, we can call `prisa()`.

``` r
fit_lm_2 <- prisa(
  main_model = fn_true_lm,
  proxy_model = fn_proxy_lm_run,
  data = df,
  labeled_set_var_name = "is_labeled",
  options = SetOptions(
    # Please increase bootstrap iterations in real use
    n_boot = 500,         
    # Turn off parallel computing to suppress CRAN warnings
    use_parallel = FALSE,
    use_full = TRUE,
  )
)

summary(fit_lm_2)
#> 
#> ── Prediction-error Robust Inference (prisa) Results ───────────────────────────
#> 
#> ── Main Estimates ──
#> 
#>             estimate std_err ci_lower_95 ci_upper_95  elss
#> (Intercept)   0.5758 0.05686      0.4643      0.6872 570.7
#> D             0.8714 0.08101      0.7126      1.0302 543.3
#> X1            1.0021 0.04451      0.9149      1.0894 551.7
#> X2            1.5096 0.04687      1.4178      1.6015 486.9
#> ── Additional Information ──
#> ── Efficiency Gain from prisa
#> • Effective Labeled Sample Size (ELSS): 570.678, 543.347, 551.683, and 486.896
#> • Contribution from the unlabeled set: 370.7, 343.3, 351.7, and 286.9
#> • Standard Error Reduction: -41, -39, -40, and -36%
#> 
#> ── Data
#> • n_labeled_data: 200
#> • n_full_data: 5200
#> • proportion_of_coded_obs: 0.0385
#> 
#> ── Labeled Only Estimates
#>             estimate std_err ci_lower_95 ci_upper_95 elss
#> (Intercept)   0.5616 0.09605      0.3733      0.7498  200
#> D             0.9478 0.13353      0.6861      1.2095  200
#> X1            1.0560 0.07392      0.9112      1.2009  200
#> X2            1.5231 0.07313      1.3798      1.6664  200
```
